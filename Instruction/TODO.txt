 📝 לעשות
 לבנות consumer קטן ב־Python שמאזין ל־MQTT ומדפיס payload.
 להחליף מקור וידאו ל־RTSP ולוודא שממשיך לפרסם.
 להוסיף דוגמאות JSON ל־README.
 להגדיר GitHub repo ולעלות את ה־README הזה כבסיס.
 
 * לבודד TEST 4 לתיקיה שבה אני אערוך אותו ויארגן את הפרוייקט
 * להאזין ב PYTHON ל MQTT
 * לחבר 4 קבצי Video ולהרכיב האזנה ל4 קבצי הוידאו דרך PYTHON
 * להרכיב כיול בין 4 קבצי הוידאו + תמונה קבוע של המפה, להוציא ערכי המרה - Affine Transfer  או הומוגראפיה ע"י אפליקציה שאני אבנה שבה אני אלביש תמונה מעל תמונה, נגיד תמונה המפה, מעליה אני אלביש את התמונה של אחד הפריימים בכך שאני אסובב אותה, אקטין, אזיז
אשמח לתוכנה מוכנה שעושה את זה, כי לדעתי זה לא מקצועי לנחש נקודות התאמה
*לקרוא את המידע המכוייל של 4 הסירטונים עם נתוני הכיול, לעשות על המידע הזה NMS ולהשאיר רק זיהויים ממוקדים על המפה הכללית.
* לתכנן איך אני אצור STREAM נוסף עם DEAPSTREAM שמקבל את המידע מ4 הסרטים יתכן בRTSP, מוסיף את נתוני הכיול, עושה NMS, ואז למידע הנקי עושה TRACKING (שאני לא יודע איך אפשר לעשות TRACKING בלי התמונה עצמה של הבן אדם, אשמח לרעיונות איך לבצע את זה), ככה על המפה הכללית גם יסומן הבן אדם וגם ימוספר בזמן שהוא זז על המפה הכללית
(שים לב: קיימים 3 אנשים, והם מאתגרים מאוד בתנועתם בכל 4 הפריימים, הם יושבים, זזים עם הכיסא, הולכים, עוזבים את הכיסא עליו ישבו, חולפים ליד השני וחלקם יוצאים מחלק מהפריימים בזמן שהם הולכים או זזים עם הכיסא)

*להוציא את הפלט על דף WEB דרך RTSP
אני צריך לתכנן איך לעשות את זה.






) TODO מעודכן (מסודר)

 לבודד את TEST4 ל־graph/ ולוודא ריצה עם MQTT.
 להריץ consumers/mqtt_consumer_minimal.py ולראות פייאלוד מגיע.
 להכין 4 מקורות RTSP (או ישירות MP4) ולהריץ 4 סורסים בגרף.
 להריץ calibration/pick_points_homography.py לכל מצלמה ולשמור H_cam*.yaml.
 להריץ consumers/fusion_tracker.py → לראות פרסום ai/global/tracks.
 להוסיף ל־README דוגמאות JSON + הוראות התקנה.
 לפתוח ריפו GitHub ולהעלות את השלד + קבצים.
 (אופציונלי) רנדר מאוחד על map בפייטון/דש או WEB.
